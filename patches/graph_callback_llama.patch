diff --git a/include/llama.h b/include/llama.h
index 1c3a1cd1..4add62ce 100644
--- a/include/llama.h
+++ b/include/llama.h
@@ -256,6 +256,8 @@ extern "C" {
         ggml_backend_buffer_type_t buft;
     };
 
+    typedef bool (*llama_graph_compute_callback)(void * data, const struct ggml_cgraph * graph, bool compute);
+
     struct llama_model_params {
         // NULL-terminated list of devices to use for offloading (if NULL, all available devices are used)
         ggml_backend_dev_t * devices;
@@ -326,6 +328,9 @@ extern "C" {
         ggml_abort_callback abort_callback;
         void *              abort_callback_data;
 
+        llama_graph_compute_callback graph_callback;
+        void *              graph_callback_data;
+
         // Keep the booleans together and at the end of the struct to avoid misalignment during copy-by-value.
         bool embeddings;  // if true, extract embeddings (together with logits)
         bool offload_kqv; // offload the KQV ops (including the KV cache) to GPU
diff --git a/src/llama-context.cpp b/src/llama-context.cpp
index 6eb34473..284edb8d 100644
--- a/src/llama-context.cpp
+++ b/src/llama-context.cpp
@@ -58,6 +58,9 @@ llama_context::llama_context(
     cparams.cb_eval           = params.cb_eval;
     cparams.cb_eval_user_data = params.cb_eval_user_data;
 
+    cparams.graph_callback = params.graph_callback;
+    cparams.graph_callback_data = params.graph_callback_data;
+
     auto rope_scaling_type = params.rope_scaling_type;
     if (rope_scaling_type == LLAMA_ROPE_SCALING_TYPE_UNSPECIFIED) {
         rope_scaling_type = hparams.rope_scaling_type_train;
@@ -721,6 +724,9 @@ llm_graph_result * llama_context::process_ubatch(const llama_ubatch & ubatch, ll
         //const auto t_start_us = ggml_time_us();
 
         gf = model.build_graph(gparams);
+        if (cparams.graph_callback) {
+            cparams.graph_callback(cparams.graph_callback_data, gf, false);
+        }
 
         //LLAMA_LOG_INFO("graph build time: %.3f ms\n", (ggml_time_us() - t_start_us)/1000.0);
 
@@ -1350,6 +1356,9 @@ ggml_cgraph * llama_context::graph_reserve(uint32_t n_tokens, uint32_t n_seqs, u
     res->reset();
 
     auto * gf = model.build_graph(gparams);
+    if (cparams.graph_callback) {
+        cparams.graph_callback(cparams.graph_callback_data, gf, false);
+    }
 
     this->n_outputs = save_n_outputs;
 
@@ -1402,6 +1411,9 @@ ggml_status llama_context::graph_compute(
         set_n_threads_fn.second(set_n_threads_fn.first, n_threads);
     }
 
+    if (cparams.graph_callback) {
+        cparams.graph_callback(cparams.graph_callback_data, gf, true);
+    }
     auto status = ggml_backend_sched_graph_compute_async(sched.get(), gf);
     if (status != GGML_STATUS_SUCCESS) {
         LLAMA_LOG_ERROR("%s: ggml_backend_sched_graph_compute_async failed with error %d\n", __func__, status);
@@ -2103,6 +2115,10 @@ void llama_context::opt_epoch_iter(
             res->reset();
 
             auto * gf = model.build_graph(gparams);
+            if (cparams.graph_callback) {
+                cparams.graph_callback(cparams.graph_callback_data, gf, false);
+            }
+
 
             struct ggml_context * ctx_compute_opt;
             {
@@ -2218,6 +2234,10 @@ llama_context_params llama_context_default_params() {
         /*.type_v                      =*/ GGML_TYPE_F16,
         /*.abort_callback              =*/ nullptr,
         /*.abort_callback_data         =*/ nullptr,
+
+        /*.graph_callback              =*/ nullptr,
+        /*.graph_callback_data         =*/ nullptr,
+
         /*.embeddings                  =*/ false,
         /*.offload_kqv                 =*/ true,
         /*.flash_attn                  =*/ false,
diff --git a/src/llama-cparams.h b/src/llama-cparams.h
index 38750aff..fd4d7185 100644
--- a/src/llama-cparams.h
+++ b/src/llama-cparams.h
@@ -39,4 +39,8 @@ struct llama_cparams {
 
     ggml_backend_sched_eval_callback cb_eval;
     void * cb_eval_user_data;
+
+    llama_graph_compute_callback graph_callback;
+    void *              graph_callback_data;
+
 };
