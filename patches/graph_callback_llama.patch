diff --git a/include/llama.h b/include/llama.h
index 01762be..de69e3b 100644
--- a/include/llama.h
+++ b/include/llama.h
@@ -289,6 +289,8 @@ extern "C" {
         ggml_backend_buffer_type_t buft;
     };
 
+    typedef bool (*llama_graph_compute_callback)(void * data, const struct ggml_cgraph * graph, bool compute);
+
     struct llama_model_params {
         // NULL-terminated list of devices to use for offloading (if NULL, all available devices are used)
         ggml_backend_dev_t * devices;
@@ -359,6 +361,9 @@ extern "C" {
         ggml_abort_callback abort_callback;
         void *              abort_callback_data;
 
+        llama_graph_compute_callback graph_callback;
+        void *              graph_callback_data;
+
         // Keep the booleans together and at the end of the struct to avoid misalignment during copy-by-value.
         bool embeddings;  // if true, extract embeddings (together with logits)
         bool offload_kqv; // offload the KQV ops (including the KV cache) to GPU
diff --git a/src/llama-context.cpp b/src/llama-context.cpp
index 62246c1..f311e11 100644
--- a/src/llama-context.cpp
+++ b/src/llama-context.cpp
@@ -51,6 +51,9 @@ llama_context::llama_context(
     cparams.cb_eval           = params.cb_eval;
     cparams.cb_eval_user_data = params.cb_eval_user_data;
 
+    cparams.graph_callback = params.graph_callback;
+    cparams.graph_callback_data = params.graph_callback_data;
+
     auto rope_scaling_type = params.rope_scaling_type;
     if (rope_scaling_type == LLAMA_ROPE_SCALING_TYPE_UNSPECIFIED) {
         rope_scaling_type = hparams.rope_scaling_type_train;
@@ -1237,7 +1240,7 @@ llm_graph_result_ptr llama_context::graph_build(
              ggml_cgraph * gf,
       const llama_ubatch & ubatch,
             llm_graph_type gtype) {
-    return model.build_graph(
+    auto res = model.build_graph(
             {
                 /*.ctx         =*/ ctx,
                 /*.arch        =*/ model.arch,
@@ -1253,6 +1256,10 @@ llm_graph_result_ptr llama_context::graph_build(
                 /*.n_outputs   =*/ n_outputs,
                 /*.cb          =*/ graph_get_cb(),
             }, gf, gtype);
+    if (cparams.graph_callback) {
+        cparams.graph_callback(cparams.graph_callback_data, gf, false);
+    }
+    return res;
 }
 
 ggml_status llama_context::graph_compute(
@@ -1271,7 +1278,9 @@ ggml_status llama_context::graph_compute(
     for (const auto & set_n_threads_fn : set_n_threads_fns) {
         set_n_threads_fn.second(set_n_threads_fn.first, n_threads);
     }
-
+    if (cparams.graph_callback) {
+        cparams.graph_callback(cparams.graph_callback_data, gf, true);
+    }
     auto status = ggml_backend_sched_graph_compute_async(sched.get(), gf);
     if (status != GGML_STATUS_SUCCESS) {
         LLAMA_LOG_ERROR("%s: ggml_backend_sched_graph_compute_async failed with error %d\n", __func__, status);
@@ -2086,6 +2095,10 @@ llama_context_params llama_context_default_params() {
         /*.type_v                      =*/ GGML_TYPE_F16,
         /*.abort_callback              =*/ nullptr,
         /*.abort_callback_data         =*/ nullptr,
+
+        /*.graph_callback              =*/ nullptr,
+        /*.graph_callback_data         =*/ nullptr,
+
         /*.embeddings                  =*/ false,
         /*.offload_kqv                 =*/ true,
         /*.flash_attn                  =*/ false,
diff --git a/src/llama-cparams.h b/src/llama-cparams.h
index 246fa57..f7f917f 100644
--- a/src/llama-cparams.h
+++ b/src/llama-cparams.h
@@ -36,4 +36,8 @@ struct llama_cparams {
 
     ggml_backend_sched_eval_callback cb_eval;
     void * cb_eval_user_data;
+
+    llama_graph_compute_callback graph_callback;
+    void *              graph_callback_data;
+
 };
